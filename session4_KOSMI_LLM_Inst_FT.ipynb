{"cells":[{"cell_type":"markdown","metadata":{"id":"R9cvXkGIWXj4"},"source":["# KOSMI 2024 Winter School\n","## Session 4. 나만의 거대언어모델 만들어 보기\n","\n","- 연자: 김준우(kjune0322@kaist.ac.kr), 권순준(sean0042@kaist.ac.kr)\n","- 발표자료: https://github.com/starmpcc/KOSMI2024-Asclepius\n","\n"]},{"cell_type":"markdown","metadata":{"id":"N3qNfAh9IYFx"},"source":["\u003ca target=\"_blank\" href=\"https://colab.research.google.com/github/starmpcc/KOSMI2024-Asclepius/blob/main/KOSMI_LLM_Inst_FT.ipynb\"\u003e\n","  \u003cimg src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/\u003e\n","\u003c/a\u003e"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":12093,"status":"ok","timestamp":1705474418956,"user":{"displayName":"EunChan Jang","userId":"09021065782462991210"},"user_tz":-540},"id":"dY38frHA5rk_"},"outputs":[],"source":["# First, install required packages\n","!pip install -q accelerate==0.25.0 peft==0.6.2 bitsandbytes==0.41.1 transformers==4.36.2 trl==0.7.4 einops gradio openai"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19763,"status":"ok","timestamp":1705474438712,"user":{"displayName":"EunChan Jang","userId":"09021065782462991210"},"user_tz":-540},"id":"cyq7qXBK5hrq","outputId":"b1c9335f-c74f-466e-fb6b-b6e28a6c2713"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n","  warnings.warn(\n"]}],"source":["# Import Libraries\n","import torch\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    TrainingArguments,\n",")\n","from peft import LoraConfig\n","from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n","import gradio as gr"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":194},"executionInfo":{"elapsed":15488,"status":"ok","timestamp":1705474454172,"user":{"displayName":"EunChan Jang","userId":"09021065782462991210"},"user_tz":-540},"id":"Xbz2wAkMV3TZ","outputId":"dd1e26e1-2177-4721-a5c5-ebebf5599e06"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8ae9e58981364ff492dc506309f18c8a","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["# To save time, first download model and data\n","\n","# Define Quantization Config\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_compute_dtype=torch.float16,\n","    bnb_4bit_quant_type=\"nf4\",\n",")\n","\n","# Load Model and Dataset\n","model = AutoModelForCausalLM.from_pretrained(\n","    \"microsoft/phi-2\",\n","    trust_remote_code=True,\n","    quantization_config=bnb_config,\n","    device_map=\"auto\",\n","    revision=\"refs/pr/23\"\n",")\n","\n","tokenizer = AutoTokenizer.from_pretrained('microsoft/phi-2')\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_sight = \"right\"\n","\n","def prompt_shorter_than(samples):\n","    concatenated = [\" \".join([i, j, k]) for i, j, k in zip(samples['note'], samples['question'], samples['answer'])]\n","    return [len(i)\u003c=320 for i in tokenizer(concatenated)['input_ids']]\n","\n","dataset = load_dataset(\"starmpcc/Asclepius-Synthetic-Clinical-Notes\")\n","dataset = dataset.filter(lambda x: [len(i)\u003c1500 for i in x['note']], batched=True)\n","dataset = dataset.filter(prompt_shorter_than, batched=True)\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1705474454172,"user":{"displayName":"EunChan Jang","userId":"09021065782462991210"},"user_tz":-540},"id":"JJxznOdbVdu6","outputId":"00ff5240-314a-4752-aa7d-f9c03433e0ba"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['patient_id', 'note', 'question', 'answer', 'task'],\n","    num_rows: 3686\n","})\n"]},{"data":{"text/plain":["{'patient_id': 110,\n"," 'note': \"Hospital Course:\\n\\nThe patient, who was involved in a murder case, was admitted to our forensic facility for toxicology testing and a complete neuropsychiatric evaluation. The case is complicated due to the defendant's claimed genetic predisposition to anti-social behavior and his regular consumption of alcohol and drugs since the beginning of adolescence.\\n\\nToxicology testing revealed detectable levels of benzoylecgonine in urine and pubic hair, while blood and saliva samples showed no significant levels of drugs or alcohol. A full clinical and neuropsychological examination was performed, which identified a personality disorder not otherwise specified. MRI imaging showed a decrease in cortical thickness with enlarged lateral ventricles, significant volumetric asymmetry of the amygdalae, and a decreased volume of the right orbito-frontal cortex in comparison with the left one. PET-CT testing did not indicate any alteration of brain perfusion or metabolism.\\n\\nBased on the tests and evaluations, it is concluded that the patient did not show any clinical signs of neurological impairment or alcohol-dependence. The patient will be transferred back to the custody of the court.\",\n"," 'question': \"What are the detectable levels of benzoylecgonine found in the patient's urine and pubic hair during toxicology testing?\",\n"," 'answer': \"Detectable levels of benzoylecgonine were found in the patient's urine and pubic hair during toxicology testing.\",\n"," 'task': 'Temporal Information Extraction'}"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Let's pre-process dataset\n","\n","# First of all, we have to check how the dataset is composed\n","print(dataset['train'])\n","dataset['train'][0]"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1705474454172,"user":{"displayName":"EunChan Jang","userId":"09021065782462991210"},"user_tz":-540},"id":"_d4zHztx-T2O","outputId":"b970e85b-bf31-4b9b-c53c-cba61ba7c76a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Instruct: Please write down your own prompt.\n","For instance, you can insert the note as {note}\n","Hospital Course:\n","\n","The patient, who was involved in a murder case, was admitted to our forensic facility for toxicology testing and a complete neuropsychiatric evaluation. The case is complicated due to the defendant's claimed genetic predisposition to anti-social behavior and his regular consumption of alcohol and drugs since the beginning of adolescence.\n","\n","Toxicology testing revealed detectable levels of benzoylecgonine in urine and pubic hair, while blood and saliva samples showed no significant levels of drugs or alcohol. A full clinical and neuropsychological examination was performed, which identified a personality disorder not otherwise specified. MRI imaging showed a decrease in cortical thickness with enlarged lateral ventricles, significant volumetric asymmetry of the amygdalae, and a decreased volume of the right orbito-frontal cortex in comparison with the left one. PET-CT testing did not indicate any alteration of brain perfusion or metabolism.\n","\n","Based on the tests and evaluations, it is concluded that the patient did not show any clinical signs of neurological impairment or alcohol-dependence. The patient will be transferred back to the custody of the court.\n","Model should answer to {question} based on the note.\n","What are the detectable levels of benzoylecgonine found in the patient's urine and pubic hair during toxicology testing?\n","You should maintain the phi-2 format\n","Accordingly, the last line must be like the below.\n","Do not forget to insert a new line between your prompt and 'Output'!\n","Output: Detectable levels of benzoylecgonine were found in the patient's urine and pubic hair during toxicology testing.\n","\n","********************\n","Prompt Length: 90 tokens\n"]}],"source":["# We make this dataset to phi-2 compatible\n","# Phi-2 instruction-answer format: \"Instruct: \u003cprompt\u003e\\nOutput:\"\n","\n","# Make your own prompt!\n","prompt_template=\"\"\"Instruct: Please write down your own prompt.\n","For instance, you can insert the note as {{note}}\n","{note}\n","Model should answer to {{question}} based on the note.\n","{question}\n","You should maintain the phi-2 format\n","Accordingly, the last line must be like the below.\n","Do not forget to insert a new line between your prompt and 'Output'!\n","Output: {answer}\n","\"\"\"\n","\n","\n","# Should get Dict[List] as input, return list of prompts\n","def format_dataset(samples):\n","    outputs = []\n","    for _, note, question, answer, _ in zip(*samples.values()):\n","        out = prompt_template.format(note=note, question=question, answer=answer)\n","        outputs.append(out)\n","    return outputs\n","\n","sample_input = format_dataset({k: [v] for k, v in dataset['train'][0].items()})[0]\n","print(sample_input)\n","print(\"*\"*20)\n","\n","# Sanity Check\n","prompt_len = len(tokenizer.encode(prompt_template))\n","if prompt_len \u003e 180:\n","    raise ValueError(f\"Your prompt is too long! Please reduce the length from {prompt_len} to 180 tokens\")\n","print(f\"Prompt Length: {prompt_len} tokens\")"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23911,"status":"ok","timestamp":1705474478061,"user":{"displayName":"EunChan Jang","userId":"09021065782462991210"},"user_tz":-540},"id":"x-F0CmI16j_l","outputId":"339de016-1ff2-4fc3-8d1c-894a42c8842a"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Instruct: Please write down your own prompt.\n","For instance, you can insert the note as {note}\n","Hospital Course:\n","\n","The patient, who was involved in a murder case, was admitted to our forensic facility for toxicology testing and a complete neuropsychiatric evaluation. The case is complicated due to the defendant's claimed genetic predisposition to anti-social behavior and his regular consumption of alcohol and drugs since the beginning of adolescence.\n","\n","Toxicology testing revealed detectable levels of benzoylecgonine in urine and pubic hair, while blood and saliva samples showed no significant levels of drugs or alcohol. A full clinical and neuropsychological examination was performed, which identified a personality disorder not otherwise specified. MRI imaging showed a decrease in cortical thickness with enlarged lateral ventricles, significant volumetric asymmetry of the amygdalae, and a decreased volume of the right orbito-frontal cortex in comparison with the left one. PET-CT testing did not indicate any alteration of brain perfusion or metabolism.\n","\n","Based on the tests and evaluations, it is concluded that the patient did not show any clinical signs of neurological impairment or alcohol-dependence. The patient will be transferred back to the custody of the court.\n","Model should answer to {question} based on the note.\n","What are the detectable levels of benzoylecgonine found in the patient's urine and pubic hair during toxicology testing?\n","You should maintain the phi-2 format\n","Accordingly, the last line must be like the below.\n","Do not forget to insert a new line between your prompt and 'Output'!\n","\n","A:\n","\n","You can use the following code:\n","#include \u003ciostream\u003e\n","#include \u003cstring\u003e\n","#include \u003cvector\u003e\n","#include \u003calgorithm\u003e\n","#include \u003citerator\u003e\n","#include \u003csstream\u003e\n","#include \u003cfstream\u003e\n","#include \u003ccstdlib\u003e\n","#include \u003cctime\u003e\n","#include \u003ccmath\u003e\n","#include \u003ccstring\u003e\n","#include \u003ccstdio\u003e\n","#include \u003ccstdlib\u003e\n","#include \u003ciomanip\u003e\n","#include \u003ccstdlib\u003e\n","#include \u003ccstring\u003e\n","#include \u003ccstdio\u003e\n","#include \u003ccstdlib\u003e\n","#include \u003ciomanip\u003e\n","#include \u003ccstring\u003e\n","#include \u003ccstdio\u003e\n","#include \u003ccstdlib\u003e\n","#include \u003c\n"]}],"source":["sample_idx = 0\n","sample_input = format_dataset({k: [v] for k, v in dataset['train'][sample_idx].items()})[0].split('Output: ')[0]\n","input_ids = tokenizer.encode(sample_input, return_tensors='pt').to('cuda')\n","with torch.no_grad():\n","  output = model.generate(input_ids=input_ids,\n","                            max_length=512,\n","                            use_cache=True,\n","                            temperature=0.,\n","                            eos_token_id=tokenizer.eos_token_id\n","  )\n","print(tokenizer.decode(output.to('cpu')[0], skip_special_tokens=True))"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":31,"status":"ok","timestamp":1705474478061,"user":{"displayName":"EunChan Jang","userId":"09021065782462991210"},"user_tz":-540},"id":"8bXTfKImbvEC"},"outputs":[],"source":["# Then, let's define dataset.\n","response_template = \"Output:\"\n","collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n","\n","train_dataset = dataset['train']\n","sampled_train_dataset = train_dataset.select(range(2000))"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1705474478061,"user":{"displayName":"EunChan Jang","userId":"09021065782462991210"},"user_tz":-540},"id":"Y8jY6PNfa8Sm","outputId":"7fc99acf-6b55-4bb2-9c18-442e1d50be02"},"outputs":[{"name":"stderr","output_type":"stream","text":["You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n"]}],"source":["# SFTTrainer Do everything else for you!\n","\n","lora_config=LoraConfig(\n","    r=4,\n","    task_type=\"CAUSAL_LM\",\n","    target_modules= [\"Wqkv\", \"fc1\", \"fc2\" ]\n",")\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    num_train_epochs=1,\n","    fp16=True,\n","    per_device_train_batch_size=2,\n","    gradient_accumulation_steps=8,\n","    learning_rate=1e-4,\n","    optim=\"paged_adamw_32bit\",\n","    save_strategy=\"no\",\n","    warmup_ratio=0.03,\n","    logging_steps=5,\n","    lr_scheduler_type=\"cosine\",\n","    report_to=\"tensorboard\",\n","    gradient_checkpointing=True\n",")\n","\n","trainer = SFTTrainer(\n","    model,\n","    training_args,\n","    train_dataset=sampled_train_dataset,\n","    formatting_func=format_dataset,\n","    data_collator=collator,\n","    peft_config=lora_config,\n","    max_seq_length=512,\n","    tokenizer=tokenizer,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":803},"id":"n_Ngq49_BA2E"},"outputs":[{"name":"stderr","output_type":"stream","text":["You're using a CodeGenTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"text/html":["\n","    \u003cdiv\u003e\n","      \n","      \u003cprogress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'\u003e\u003c/progress\u003e\n","      [125/125 22:58, Epoch 1/1]\n","    \u003c/div\u003e\n","    \u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n"," \u003ctr style=\"text-align: left;\"\u003e\n","      \u003cth\u003eStep\u003c/th\u003e\n","      \u003cth\u003eTraining Loss\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e5\u003c/td\u003e\n","      \u003ctd\u003e0.700500\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e10\u003c/td\u003e\n","      \u003ctd\u003e0.653100\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e15\u003c/td\u003e\n","      \u003ctd\u003e0.604900\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e20\u003c/td\u003e\n","      \u003ctd\u003e0.550800\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e25\u003c/td\u003e\n","      \u003ctd\u003e0.486300\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e30\u003c/td\u003e\n","      \u003ctd\u003e0.465200\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e35\u003c/td\u003e\n","      \u003ctd\u003e0.512800\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e40\u003c/td\u003e\n","      \u003ctd\u003e0.601100\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e45\u003c/td\u003e\n","      \u003ctd\u003e0.579700\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e50\u003c/td\u003e\n","      \u003ctd\u003e0.512100\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e55\u003c/td\u003e\n","      \u003ctd\u003e0.614800\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e60\u003c/td\u003e\n","      \u003ctd\u003e0.550000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e65\u003c/td\u003e\n","      \u003ctd\u003e0.495600\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e70\u003c/td\u003e\n","      \u003ctd\u003e0.466800\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e75\u003c/td\u003e\n","      \u003ctd\u003e0.441600\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e80\u003c/td\u003e\n","      \u003ctd\u003e0.452300\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e85\u003c/td\u003e\n","      \u003ctd\u003e0.587400\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e90\u003c/td\u003e\n","      \u003ctd\u003e0.514900\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e95\u003c/td\u003e\n","      \u003ctd\u003e0.472100\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e100\u003c/td\u003e\n","      \u003ctd\u003e0.596200\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e105\u003c/td\u003e\n","      \u003ctd\u003e0.498500\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e110\u003c/td\u003e\n","      \u003ctd\u003e0.525500\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e115\u003c/td\u003e\n","      \u003ctd\u003e0.490300\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e120\u003c/td\u003e\n","      \u003ctd\u003e0.491100\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e125\u003c/td\u003e\n","      \u003ctd\u003e0.471900\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\u003cp\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=125, training_loss=0.5334161357879639, metrics={'train_runtime': 1388.8048, 'train_samples_per_second': 1.44, 'train_steps_per_second': 0.09, 'total_flos': 1.237657008494592e+16, 'train_loss': 0.5334161357879639, 'epoch': 1.0})"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Run Training\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"c_ntjMAe6Uoh"},"outputs":[{"name":"stdout","output_type":"stream","text":["Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","Running on public URL: https://86d6f91a8d7f0bfa0d.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"]},{"data":{"text/html":["\u003cdiv\u003e\u003ciframe src=\"https://86d6f91a8d7f0bfa0d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen\u003e\u003c/iframe\u003e\u003c/div\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":[]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# Wrap-up Training\n","model = trainer.model\n","model.eval()\n","\n","note_samples = train_dataset.select(range(len(train_dataset)-10, len(train_dataset)))['note']\n","\n","def inference(note, question, model):\n","    prompt = prompt_template.format(note=note, question=question, answer=\"\")\n","    tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to('cuda')\n","    outs = model.generate(input_ids=tokens,\n","                          max_length=512,\n","                          use_cache=True,\n","                          temperature=0.,\n","                          eos_token_id=tokenizer.eos_token_id\n","                          )\n","    output_text = tokenizer.decode(outs.to('cpu')[0], skip_special_tokens=True)\n","    return output_text[len(prompt):]\n","\n","\n","def compare_models(note, question):\n","    with torch.no_grad():\n","        asc_answer = inference(note, question, trainer.model)\n","        with model.disable_adapter():\n","            phi_answer = inference(note, question, trainer.model)\n","    return asc_answer, phi_answer\n","\n","demo = gr.Interface(fn=compare_models, inputs=[gr.Dropdown(note_samples), \"text\"], outputs=[gr.Textbox(label=\"Asclepius\"), gr.Textbox(label=\"Phi-2\")])\n","demo.launch()"]}],"metadata":{"accelerator":"GPU","colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1d17b256eb7e42129b87a3ed2a966526":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7fb910d838374b318c00780b8366c7c9","placeholder":"​","style":"IPY_MODEL_a7ca9a909fff4b61b9690a7589a5835e","value":" 2/2 [00:11\u0026lt;00:00,  4.75s/it]"}},"27e9b94a3ac54c55a3e795eb2873db51":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d09d577eeee4710b851a61ade3788d7","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_77ed9f59d5bb4204af697a089b446e02","value":2}},"3806113a75824d2a8840a779829a49bd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d09d577eeee4710b851a61ade3788d7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"498b27a09226430b99fe826c241f14b3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3806113a75824d2a8840a779829a49bd","placeholder":"​","style":"IPY_MODEL_db8aeb6c610343a8b978e994f1f1e2fe","value":"Loading checkpoint shards: 100%"}},"62ea108dc5d740d29c631a9990bb21f2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77ed9f59d5bb4204af697a089b446e02":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7fb910d838374b318c00780b8366c7c9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ae9e58981364ff492dc506309f18c8a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_498b27a09226430b99fe826c241f14b3","IPY_MODEL_27e9b94a3ac54c55a3e795eb2873db51","IPY_MODEL_1d17b256eb7e42129b87a3ed2a966526"],"layout":"IPY_MODEL_62ea108dc5d740d29c631a9990bb21f2"}},"a7ca9a909fff4b61b9690a7589a5835e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"db8aeb6c610343a8b978e994f1f1e2fe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}